POMDPs.action(p::T, b; i) where T<:Policy = action(p, b)

struct BestCurrentOption <: Policy
    pomdp::InfoGatheringPOMDP
end

function POMDPs.action(p::BestCurrentOption, b::DiscreteBelief)
    action_values = [sum([b*reward(p.pomdp, s, a) for (b, s) in zip(b.b, p.pomdp.states)]) for a in p.pomdp.terminal_actions]
    return p.pomdp.terminal_actions[argmax(action_values)]
end

@with_kw struct EnsureParticleCount <: Policy
    policy::Policy
    final_action_policy::Policy
    min_particle_count::Int = 50
end

function POMDPs.action(p::EnsureParticleCount, b::DiscreteBelief; i=nothing)
    if sum(b.b .> 0) <= p.min_particle_count
        return action(p.final_action_policy, b; i)
    else
        return action(p.policy, b; i)
    end
end

struct FixedPolicy <: Policy
    actions::Vector
    backup_policy::Policy 
    FixedPolicy(actions, backup_policy = FunctionPolicy((b)->error("No action defined for this policy."))) = new(actions, backup_policy)
end

function POMDPs.action(p::FixedPolicy, b; i=nothing)
    if i > length(p.actions)
        a = action(p.backup_policy, b; i)
    else
        a = p.actions[i]
    end
    return a
end

@with_kw struct RandPolicy <: Policy
    prob_terminal::Float64 = 0.1
    pomdp::InfoGatheringPOMDP
    best_current_option::BestCurrentOption = BestCurrentOption(pomdp)
end

function POMDPs.action(p::RandPolicy, b)
    if rand() < p.prob_terminal
        return action(p.best_current_option, b)
    else
        return rand(setdiff(actions(p.pomdp), p.pomdp.terminal_actions))
    end
end

@with_kw struct OneStepGreedyPolicy <: Policy
    pomdp::InfoGatheringPOMDP
end

function lookahead(ğ’«, U, b, a, up)
    r = sum(reward(ğ’«, s, a)*b.b[i] for (i,s) in enumerate(states(ğ’«))) 
    Posa(o,s,a) = sum(obs_weight(ğ’«, s, a, sâ€², o)*psâ€² for (sâ€², psâ€²) in transition(ğ’«, s, a)) 
    Poba(o,b,a) = sum(b.b[i]*Posa(o,s,a) for (i,s) in enumerate(states(ğ’«)))
    return r + discount(ğ’«, a)*sum([Poba(o,b,a)*U(update(up, b, a, o).b) for o in observations(ğ’«, a)], init=0) 
end 

function greedy(ğ’«, U, b) 
    As = actions(ğ’«)
    up = DiscreteUp(ğ’«)
    u, a = findmax(a->lookahead(ğ’«, U, b, a, up), As) 
    return (a=As[a], u=u) 
end 

function greedy(Ï€, b) 
    U(b) = utility(Ï€, b) 
    return greedy(Ï€.pomdp, U, b) 
end

function utility(Ï€::OneStepGreedyPolicy, b)
    return maximum([b â‹… [reward(Ï€.pomdp, s, a) for s in states(Ï€.pomdp)] for a in Ï€.pomdp.terminal_actions])
end

POMDPs.action(Ï€::OneStepGreedyPolicy, b::DiscreteBelief) = greedy(Ï€, b).a

POMDPs.value(Ï€::OneStepGreedyPolicy, b::DiscreteBelief) = greedy(Ï€, b).u

POMDPs.value(Ï€::OneStepGreedyPolicy, b::DiscreteBelief, a) = lookahead(Ï€.pomdp, Ï€, b, a, DiscreteUp(Ï€.pomdp))


function POMDPTools.actionvalues(Ï€::OneStepGreedyPolicy, b::DiscreteBelief)
    U(b) = utility(Ï€, b) 
    up = DiscreteUp(Ï€.pomdp)
    [lookahead(Ï€.pomdp, U, b, a, up) for a in actions(Ï€.pomdp)]
end

## Lower bounds for SARSOP:

# Lower bound where you always walk away
struct WalkAwayNextLB <: Solver
end

function POMDPs.solve(sol::WalkAwayNextLB, pomdp)
    (;R,T) = pomdp
    S = states(pomdp)
    A = actions(pomdp)

    Î“ = [zeros(length(S)) for _ in eachindex(A)]
    for a âˆˆ A
        for s âˆˆ S
            Î“[a][s] = R[s, a]
        end
    end

    return AlphaVectorPolicy(pomdp, Î“, A)
end

# Essentially policy evaluation of the greedy policy after taking action a
function alpha_a(pomdp, a, b0 = initialstate(pomdp))
    #Function to compute the likelihood of observtion o when in state s and take action a
    Posa(o,s,a) = sum(obs_weight(pomdp, s, a, sâ€², o)*psâ€² for (sâ€², psâ€²) in transition(pomdp, s, a))
    S = states(pomdp)

    # Function that returns the terminal action that has the highest expected value under the updated belief
    up = DiscreteUp(pomdp)
    updates = Dict(o => update(up, b0, a, o) for o in observations(pomdp, a))
    rsa = Dict(a => [reward(pomdp, s, a) for s in S] for a in pomdp.terminal_actions)
    function aâ€²(o)
        bâ€² = updates[o]
        ai = argmax([bâ€².b â‹… rsa[a] for a in pomdp.terminal_actions])
        return pomdp.terminal_actions[ai]
    end

    UÏ€(s) = reward(pomdp, s, a) + discount(pomdp, a)*sum([Posa(o,s,a)*reward(pomdp, s, aâ€²(o)) for o in observations(pomdp, a)], init=0)
    return [UÏ€(s) for s in S]
end

function onestep_alphavec_policy(pomdp, b0=initialstate(pomdp))
    A = actions(pomdp)
    Î“ = [alpha_a(pomdp, a, b0) for a in A]
    return AlphaVectorPolicy(pomdp, Î“, 1:length(A))
end

## If you have an alphavector policy already, this will return it when solve is called, so that you don't have to handle this modified sparse tabular thing
struct PreSolved
    Î±vecÏ€
end

POMDPs.solve(solver::PreSolved, pomdp) = solver.Î±vecÏ€

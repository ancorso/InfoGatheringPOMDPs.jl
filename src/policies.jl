POMDPs.action(p::T, b; i) where T<:Policy = action(p, b)

struct BestCurrentOption <: Policy
    pomdp::InfoGatheringPOMDP
end

function POMDPs.action(p::BestCurrentOption, b::DiscreteBelief)
    action_values = [sum([b*reward(p.pomdp, s, a) for (b, s) in zip(b.b, p.pomdp.states)]) for a in p.pomdp.terminal_actions]
    return p.pomdp.terminal_actions[argmax(action_values)]
end

@with_kw struct EnsureParticleCount <: Policy
    policy::Policy
    final_action_policy::Policy
    min_particle_count::Int = 50
end

function POMDPs.action(p::EnsureParticleCount, b::DiscreteBelief; i=nothing)
    if sum(b.b .> 0) <= p.min_particle_count
        return action(p.final_action_policy, b; i)
    else
        return action(p.policy, b; i)
    end
end

struct FixedPolicy <: Policy
    actions::Vector
    backup_policy::Policy 
    FixedPolicy(actions, backup_policy = FunctionPolicy((b)->error("No action defined for this policy."))) = new(actions, backup_policy)
end

function POMDPs.action(p::FixedPolicy, b; i=nothing)
    if i > length(p.actions)
        a = action(p.backup_policy, b; i)
    else
        a = p.actions[i]
    end
    return a
end

@with_kw struct RandPolicy <: Policy
    prob_terminal::Float64 = 0.1
    pomdp::InfoGatheringPOMDP
    best_current_option::BestCurrentOption = BestCurrentOption(pomdp)
end

function POMDPs.action(p::RandPolicy, b)
    if rand() < p.prob_terminal
        return action(p.best_current_option, b)
    else
        return rand(setdiff(actions(p.pomdp), p.pomdp.terminal_actions))
    end
end

@with_kw struct OneStepGreedyPolicy <: Policy
    pomdp::InfoGatheringPOMDP
end

function lookahead(ğ’«, U, b, a, up)
    r = sum(reward(ğ’«, s, a)*b.b[i] for (i,s) in enumerate(states(ğ’«))) 
    Posa(o,s,a) = sum(obs_weight(ğ’«, s, a, sâ€², o)*psâ€² for (sâ€², psâ€²) in transition(ğ’«, s, a)) 
    Poba(o,b,a) = sum(b.b[i]*Posa(o,s,a) for (i,s) in enumerate(states(ğ’«)))
    return r + discount(ğ’«, a)*sum([Poba(o,b,a)*U(update(up, b, a, o).b) for o in observations(ğ’«, a)], init=0) 
end 

function greedy(ğ’«, U, b) 
    As = actions(ğ’«)
    up = DiscreteUp(ğ’«)
    u, a = findmax(a->lookahead(ğ’«, U, b, a, up), As) 
    return (a=As[a], u=u) 
end 

function greedy(Ï€, b) 
    U(b) = utility(Ï€, b) 
    return greedy(Ï€.pomdp, U, b) 
end

function utility(Ï€::OneStepGreedyPolicy, b)
    return maximum([b â‹… [reward(Ï€.pomdp, s, a) for s in states(Ï€.pomdp)] for a in Ï€.pomdp.terminal_actions])
end

POMDPs.action(Ï€::OneStepGreedyPolicy, b::DiscreteBelief) = greedy(Ï€, b).a
